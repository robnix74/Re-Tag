{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-and-Analyze\" data-toc-modified-id=\"Load-and-Analyze-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load and Analyze</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preprocessing</a></span></li><li><span><a href=\"#Feature-Engineering-for-traditional-ML\" data-toc-modified-id=\"Feature-Engineering-for-traditional-ML-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Feature Engineering for traditional ML</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>TF-IDF</a></span></li></ul></li><li><span><a href=\"#Traditional-ML-Algorithms\" data-toc-modified-id=\"Traditional-ML-Algorithms-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Traditional ML Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Naive Bayes</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Random Forest</a></span></li><li><span><a href=\"#Non-Linear-SVM\" data-toc-modified-id=\"Non-Linear-SVM-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Non Linear SVM</a></span></li><li><span><a href=\"#Linear-SVM\" data-toc-modified-id=\"Linear-SVM-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Linear SVM</a></span></li></ul></li><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Word Embeddings</a></span></li><li><span><a href=\"#Deep-Learning-Algorithms\" data-toc-modified-id=\"Deep-Learning-Algorithms-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Deep Learning Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#CNN\" data-toc-modified-id=\"CNN-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>CNN</a></span></li><li><span><a href=\"#CNN-Multi-Layer\" data-toc-modified-id=\"CNN-Multi-Layer-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>CNN Multi Layer</a></span></li><li><span><a href=\"#RNN\" data-toc-modified-id=\"RNN-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>RNN</a></span></li><li><span><a href=\"#CNN-+-LSTM-Stacking\" data-toc-modified-id=\"CNN-+-LSTM-Stacking-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>CNN + LSTM Stacking</a></span></li></ul></li><li><span><a href=\"#Training-and-Validation-Process\" data-toc-modified-id=\"Training-and-Validation-Process-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Training and Validation Process</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word2Vec-learnt-from-data\" data-toc-modified-id=\"Word2Vec-learnt-from-data-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Word2Vec learnt from data</a></span></li><li><span><a href=\"#Glove-Pretrained\" data-toc-modified-id=\"Glove-Pretrained-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Glove Pretrained</a></span></li></ul></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('wordnet') # Uncomment when running for the first time.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "!pip install scikit-multilearn\n",
    "import skmultilearn\n",
    "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Dropout, SpatialDropout1D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3l95HZo-uOj"
   },
   "source": [
    "## Load and Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ci7v23vT-msJ"
   },
   "outputs": [],
   "source": [
    "source_path = '/content/drive/My Drive/AV_Independence/'\n",
    "submission_path = '/content/drive/My Drive/AV_Independence/Submissions/'\n",
    "data = pd.read_csv('/content/drive/My Drive/AV_Independence/train.csv')\n",
    "test = pd.read_csv('/content/drive/My Drive/AV_Independence/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "Yv5WY1X4-sSq",
    "outputId": "cc30c554-4e9f-4be0-ce84-11ba52b2a185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20972, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  ... Quantitative Finance\n",
       "0   1  ...                    0\n",
       "1   2  ...                    0\n",
       "2   3  ...                    0\n",
       "3   4  ...                    0\n",
       "4   5  ...                    0\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "kbnfs-er-5fO",
    "outputId": "1c6369d4-c86c-4bb4-bb2c-31091d0f9768"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                      0\n",
       "TITLE                   0\n",
       "ABSTRACT                0\n",
       "Computer Science        0\n",
       "Physics                 0\n",
       "Mathematics             0\n",
       "Statistics              0\n",
       "Quantitative Biology    0\n",
       "Quantitative Finance    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for NULL values\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "colab_type": "code",
    "id": "_CaoUZW9-5kc",
    "outputId": "70b81f58-4900-4a0f-fcef-ea8e5ffbe6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science        8594\n",
      "Physics                 6013\n",
      "Mathematics             5618\n",
      "Statistics              5206\n",
      "Quantitative Biology     587\n",
      "Quantitative Finance     249\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f93f8f2e780>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output Label Distribution:\n",
    "\n",
    "print(data.iloc[:,3:].sum(axis = 0))\n",
    "data.iloc[:,3:].sum(axis = 0).plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LoAMwlz9-Icm"
   },
   "source": [
    "Computer Science has the highest frequency distribution and Quantitative Finance has the least.\n",
    "\n",
    "The occurences of QB and QF are very low and thus might cause issues with imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gV6pSPcE-Icp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of datapoints with multiple labels. A new dataframe which is a subset of datapoints which consists of multiple labels is defined for analysis.\n",
    "\n",
    "multilabel_da = data[data.iloc[:,3:].sum(axis = 1) > 1]\n",
    "multilabel_da['Labels_Sum'] = multilabel_da.iloc[:, 4:].sum(axis = 1)\n",
    "multilabel_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8ioqGUa-Ics",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maximum_multi_label = multilabel_da['Labels_Sum'].max()\n",
    "print('Maximum number of labels associated with a datapoint : ', maximum_multi_label)\n",
    "\n",
    "multilabel_da[multilabel_da['Labels_Sum'] == 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1e4-RhD-Icz"
   },
   "source": [
    "Maximum number of labels associated with a datapoint is 3.\n",
    "\n",
    "There are 10 such datapoints and 9 of them belong to the multilabel class {Physics, Mathematics. Statistics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "bRFfa8QJ-Icz",
    "outputId": "1726c158-2b6f-4850-b669-575e6969861d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                               16481205\n",
       "TITLE                   Many-Body Localization: Stability and Instabil...\n",
       "ABSTRACT                  Rare regions with weak disorder (Griffiths r...\n",
       "Computer Science                                                      241\n",
       "Physics                                                               447\n",
       "Mathematics                                                          1316\n",
       "Statistics                                                           1275\n",
       "Quantitative Biology                                                  114\n",
       "Quantitative Finance                                                   30\n",
       "Labels_Sum                                                           3182\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_da[multilabel_da['Labels_Sum'] == 2].sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6MiZP2Qy-Ic2"
   },
   "source": [
    "Additional Possible Inisghts:\n",
    "\n",
    "Most Paired Combo\n",
    "Least Paired Combo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tWqFZnMu-Ic9"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APoq2tf6bt-V"
   },
   "outputs": [],
   "source": [
    "y_train = np.array(data.iloc[:,3:])\n",
    "output_labels = list(data.columns[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROqMrp05-Ic9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine TITLE and ABSTRACT columns into a new column named Text\n",
    "\n",
    "data['Text'] = data.TITLE +  data.ABSTRACT\n",
    "test['Text'] = test.TITLE +  test.ABSTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qA-QZFMB-IdC"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    punctuations = string.punctuation.replace(\"\", \" \").split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    #first_stop = [i for i in stopwords_list if len(i) <= 3 or i.endswith('re') or i.find('\\'') != -1]\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"[-:,()\\\"\\']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    clean_words = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in punctuations and tok not in stopwords_list]\n",
    "    \n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HC4BTBzO-IdE"
   },
   "outputs": [],
   "source": [
    "train_clean = data['Text'].apply(preprocess)\n",
    "test_clean = test['Text'].apply(preprocess)\n",
    "\n",
    "X_train_dl, X_valid_dl, y_train_dl, y_valid_dl = train_test_split(train_clean, \n",
    "                                                    y_train, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42,)\n",
    "\n",
    "X_train_dl = [i.split() for i in X_train_dl]\n",
    "X_valid_dl = [i.split() for i in X_valid_dl]\n",
    "\n",
    "X_test_dl = [i.split() for i in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "6jbLuO6bQ6A3",
    "outputId": "4ee00454-5973-4f1b-9164-0441e177c20d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7741 5390 5038 4690  529  223]\n",
      "[853 623 580 516  58  26]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(y_train_dl).sum(axis=0))\n",
    "print(np.array(y_valid_dl).sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPgS2fmn-IdI"
   },
   "source": [
    "## Feature Engineering for traditional ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnO-EkfHCUWo"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:33:51.118574Z",
     "start_time": "2020-08-20T13:33:51.046793Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "p3n4X03q-5c0",
    "outputId": "cd98812c-d8e6-4378-c21b-218a936ceefa"
   },
   "outputs": [],
   "source": [
    "tfidf_pipeline = Pipeline([('tfidf_vectorizer', TfidfVectorizer(max_features = 7000))\n",
    "                          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T12:52:55.040111Z",
     "start_time": "2020-08-20T12:52:55.028142Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8xPdFsEXijiL"
   },
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "\n",
    "def get_baseline_pipeline(algorithm_index):\n",
    "    ML_algorithms = [LogisticRegression(random_state=42), GaussianNB(), tree.DecisionTreeClassifier(random_state=42), \n",
    "                   RandomForestClassifier(random_state=42), SVC(random_state=42), SGDClassifier(random_state=42, loss='hinge', penalty='l2',max_iter=5, tol=None)\n",
    "                  ]\n",
    "\n",
    "    OVR_pipeline = Pipeline([\n",
    "                          ('tfidf_pipeline', tfidf_pipeline),\n",
    "                          ('to_dense', DenseTransformer()),\n",
    "                          ('clf', OneVsRestClassifier(ML_algorithms[algorithm_index], n_jobs=-1))\n",
    "                          ])\n",
    "\n",
    "    BR_pipeline = Pipeline([\n",
    "                          ('tfidf_pipeline', tfidf_pipeline),\n",
    "                          ('to_dense', DenseTransformer()),\n",
    "                          ('clf', BinaryRelevance(ML_algorithms[algorithm_index]))\n",
    "                          ])\n",
    "\n",
    "    CC_pipeline = Pipeline([\n",
    "                          ('tfidf_pipeline', tfidf_pipeline),\n",
    "                          ('to_dense', DenseTransformer()),\n",
    "                          ('clf', ClassifierChain(ML_algorithms[algorithm_index]))\n",
    "                          ])\n",
    "\n",
    "    LP_pipeline = Pipeline([\n",
    "                          ('tfidf_pipeline', tfidf_pipeline),\n",
    "                          ('to_dense', DenseTransformer()),\n",
    "                          ('clf', LabelPowerset(ML_algorithms[algorithm_index]))\n",
    "                          ])\n",
    "\n",
    "    return [OVR_pipeline, BR_pipeline, CC_pipeline, LP_pipeline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:21:08.017364Z",
     "start_time": "2020-08-20T13:21:08.012344Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_OVR_overall_accuracy(y_true, y_pred):\n",
    "    return round(((list((y_true == y_pred).sum(axis = 1)).count(6)) / y_true.shape[0]), 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T12:52:55.125880Z",
     "start_time": "2020-08-20T12:52:55.120894Z"
    }
   },
   "outputs": [],
   "source": [
    "ML_algo_names = ['Logistic', 'GaussianNB', 'DecisionTree', 'RandomForest', 'SVM_Non_Linear', 'SVM_Linear']\n",
    "methods = ['OVR', 'BR', 'CC', 'LP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WF0BcavfDBuF"
   },
   "source": [
    "## Traditional ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the algorithms can be enclosed in a single loop. But, since each algorithm took a lot of time to execute, separate cells have been added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T13:45:20.340592Z",
     "start_time": "2020-08-20T13:40:52.845949Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid_Logistic_BR\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-64e3943a7c98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mlogistic_grid_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mlogistic_grid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mlogistic_grid_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'train_score'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mlogistic_grid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mlogistic_grid_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'best_score'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mlogistic_grid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 253\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\skmultilearn\\problem_transform\\br.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0my_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             classifier.fit(self._ensure_input_format(\n\u001b[1;32m--> 162\u001b[1;33m                 X), self._ensure_output_format(y_subset))\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1415\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1417\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "algorithm_index = 0\n",
    "\n",
    "logistic_grid_results = {}\n",
    "logistic_grid_pipeline = get_baseline_pipeline(algorithm_index)\n",
    "\n",
    "for method_index in range(len(methods)): # CHange 1 to zero\n",
    "    suffix = 'Grid_' + ML_algo_names[algorithm_index] + '_' + methods[method_index]\n",
    "    print(suffix)\n",
    "    \n",
    "    logistic_grid_results[suffix] = {}\n",
    "    \n",
    "    if method_index == 0:\n",
    "        clf_param_name = 'estimator'\n",
    "    else:\n",
    "        clf_param_name = 'classifier'\n",
    "        \n",
    "    logistic_param_grid = {\n",
    "        'clf__' + clf_param_name + '__solver' : ['newton-cg', 'saga'],\n",
    "        'clf__' + clf_param_name + '__penalty' : ['l2'],\n",
    "        'clf__' + clf_param_name + '__C' : [0.7, 0.8, 0.9],\n",
    "        'clf__' + clf_param_name + '__max_iter' : [1000, 1500],\n",
    "        'clf__' + clf_param_name + '__n_jobs' : [-1],  #CHANGE YOUR N_JOBS HERE\n",
    "        'tfidf_pipeline__tfidf_vectorizer__ngram_range' : [(1,1), (1,2), (1,3)]\n",
    "    }  \n",
    "    \n",
    "    cv_split = ShuffleSplit(n_splits = 3, test_size = .1, random_state = 42)\n",
    "    \n",
    "    logistic_grid_search = GridSearchCV(\n",
    "        estimator=logistic_grid_pipeline[method_index], param_grid=logistic_param_grid,\n",
    "        scoring='accuracy', cv=cv_split, verbose=1, n_jobs=-1, refit=True\n",
    "    )\n",
    "\n",
    "    if method_index == 0:\n",
    "        class_predictions = []\n",
    "        test_predictions = []\n",
    "        for label in output_labels:\n",
    "            logistic_grid_results[suffix + '_' + label] = {}\n",
    "            print('... Processing {}'.format(label))\n",
    "            logistic_grid_search.fit(X_train, np.array(data[label]))\n",
    "            print('Train Score : ', logistic_grid_search.score(X_train, np.array(data[label])))\n",
    "            \n",
    "            logistic_grid_results[suffix + '_' + label].update({'train_score' : logistic_grid_search.score(X_train, np.array(data[label]))})\n",
    "            logistic_grid_results[suffix + '_' + label].update({'best_params' : logistic_grid_search.best_params_})\n",
    "            logistic_grid_results[suffix + '_' + label].update({'best_estimator_score' : logistic_grid_search.best_estimator_.score(X_train, np.array(data[label]))})\n",
    "            \n",
    "            class_predictions.append(logistic_grid_search.predict(X_train))\n",
    "            test_predictions.append(logistic_grid_search.predict(X_test))\n",
    "        \n",
    "        pd.DataFrame(np.array(class_predictions)).T.to_csv(suffix + '_' + \"OVR.csv\")\n",
    "        pd.DataFrame(np.array(test_predictions)).T.to_csv(\"test_pred_\" + suffix + '_' + \"OVR.csv\")\n",
    "        logistic_grid_results[suffix + '_' + 'OVR'] = {}\n",
    "        logistic_grid_results[suffix + '_' + 'OVR'].update({'full_train_score' : calculate_OVR_overall_accuracy(y_train, np.array(class_predictions).T)})\n",
    "            \n",
    "    else:\n",
    "        logistic_grid_results[suffix] = {}\n",
    "        logistic_grid_search.fit(X_train, y_train)\n",
    "        logistic_grid_results[suffix].update({'train_score' : logistic_grid_search.score(X_train, y_train)})\n",
    "        logistic_grid_results[suffix].update({'best_score' : logistic_grid_search.best_score_})\n",
    "        logistic_grid_results[suffix].update({'best_params' : logistic_grid_search.best_params_})\n",
    "        logistic_grid_results[suffix].update({'train_pred' : logistic_grid_search.predict(X_train)})\n",
    "        logistic_grid_results[suffix].update({'F1 Score' : f1_score(y_train, logistic_grid_results[suffix]['train_pred'], average='micro')})\n",
    "        logistic_grid_results[suffix].update({'Precision' : precision_score(y_train, logistic_grid_results[suffix]['train_pred'])})\n",
    "        logistic_grid_results[suffix].update({'Recall' : recall_score(y_train, logistic_grid_results[suffix]['train_pred'])})\n",
    "                                              \n",
    "        logistic_grid_results[suffix].update({'best_estimator_score' : logistic_grid_search.best_estimator_.score(X_train, y_train)})\n",
    "        \n",
    "        try:\n",
    "            pd.DataFrame.sparse.from_spmatrix(logistic_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "        except:\n",
    "            print('Sparse failed for ',suffix)\n",
    "            try:\n",
    "                pd.DataFrame(logistic_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "            except:\n",
    "                print('Normal CSV also failed for', suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T15:10:41.618791Z",
     "start_time": "2020-08-20T14:47:56.344382Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid_GaussianNB_OVR\n",
      "... Processing Computer Science\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:  9.7min remaining: 12.1min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-5a58004eb980>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mgnb_grid_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msuffix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'... Processing {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mgnb_grid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train Score : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgnb_grid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gaussian NB\n",
    "\n",
    "algorithm_index = 1\n",
    "\n",
    "gnb_grid_results = {}\n",
    "gnb_grid_pipeline = get_baseline_pipeline(algorithm_index)\n",
    "\n",
    "for method_index in range(len(methods)): # CHange 1 to zero\n",
    "    suffix = 'Grid_' + ML_algo_names[algorithm_index] + '_' + methods[method_index]\n",
    "    print(suffix)\n",
    "    \n",
    "    gnb_grid_results[suffix] = {}\n",
    "    \n",
    "    if method_index == 0:\n",
    "        clf_param_name = 'estimator'\n",
    "    else:\n",
    "        clf_param_name = 'classifier'\n",
    "        \n",
    "    gnb_param_grid = {\n",
    "        'tfidf_pipeline__tfidf_vectorizer__ngram_range' : [(1,1), (1,2), (1,3)]\n",
    "    }\n",
    "    \n",
    "    cv_split = ShuffleSplit(n_splits = 3, test_size = .1, random_state = 42)\n",
    "    \n",
    "    gnb_grid_search = GridSearchCV(\n",
    "        estimator=gnb_grid_pipeline[method_index], param_grid=gnb_param_grid,\n",
    "        scoring='accuracy', cv=cv_split, verbose=1, n_jobs=-1, refit=True\n",
    "    )\n",
    "\n",
    "    if method_index == 0:\n",
    "        class_predictions = []\n",
    "        test_predictions = []\n",
    "        for label in output_labels:\n",
    "            gnb_grid_results[suffix + '_' + label] = {}\n",
    "            print('... Processing {}'.format(label))\n",
    "            gnb_grid_search.fit(X_train, np.array(data[label]))\n",
    "            print('Train Score : ', gnb_grid_search.score(X_train, np.array(data[label])))\n",
    "            \n",
    "            gnb_grid_results[suffix + '_' + label].update({'train_score' : gnb_grid_search.score(X_train, np.array(data[label]))})\n",
    "            gnb_grid_results[suffix + '_' + label].update({'best_params' : gnb_grid_search.best_params_})\n",
    "            gnb_grid_results[suffix + '_' + label].update({'best_estimator_score' : gnb_grid_search.best_estimator_.score(X_train, np.array(data[label]))})\n",
    "            \n",
    "            class_predictions.append(gnb_grid_search.predict(X_train))\n",
    "            test_predictions.append(gnb_grid_search.predict(X_test))\n",
    "            \n",
    "        pd.DataFrame(np.array(class_predictions)).T.to_csv(suffix + '_' + \"OVR.csv\")\n",
    "        pd.DataFrame(np.array(test_predictions)).T.to_csv(\"test_pred_\" + suffix + '_' + \"OVR.csv\")\n",
    "        gnb_grid_results[suffix + '_' + 'OVR'] = {}\n",
    "        gnb_grid_results[suffix + '_' + 'OVR'].update({'full_train_score' : calculate_OVR_overall_accuracy(y_train, np.array(class_predictions).T)})\n",
    "            \n",
    "    else:\n",
    "        gnb_grid_results[suffix] = {}\n",
    "        gnb_grid_search.fit(X_train, y_train)\n",
    "        gnb_grid_results[suffix].update({'train_score' : gnb_grid_search.score(X_train, y_train)})\n",
    "        gnb_grid_results[suffix].update({'best_score' : gnb_grid_search.best_score_})\n",
    "        gnb_grid_results[suffix].update({'best_params' : gnb_grid_search.best_params_})\n",
    "        gnb_grid_results[suffix].update({'train_pred' : gnb_grid_search.predict(X_train)})\n",
    "        gnb_grid_results[suffix].update({'F1 Score' : f1_score(y_train, gnb_grid_results[suffix]['train_pred'], average='micro')})\n",
    "        gnb_grid_results[suffix].update({'Precision' : precision_score(y_train, gnb_grid_results[suffix]['train_pred'])})\n",
    "        gnb_grid_results[suffix].update({'Recall' : recall_score(y_train, gnb_grid_results[suffix]['train_pred'])})\n",
    "                                              \n",
    "        gnb_grid_results[suffix].update({'best_estimator_score' : gnb_grid_search.best_estimator_.score(X_train, y_train)})\n",
    "        \n",
    "        try:\n",
    "            pd.DataFrame.sparse.from_spmatrix(gnb_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "        except:\n",
    "            print('Sparse failed for ',suffix)\n",
    "            try:\n",
    "                pd.DataFrame(gnb_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "            except:\n",
    "                print('Normal CSV also failed for', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "algorithm_index = 2\n",
    "\n",
    "dt_grid_results = {}\n",
    "dt_grid_pipeline = get_baseline_pipeline(algorithm_index)\n",
    "\n",
    "for method_index in range(len(methods)): # CHange 1 to zero\n",
    "    suffix = 'Grid_' + ML_algo_names[algorithm_index] + '_' + methods[method_index]\n",
    "    print(suffix)\n",
    "    \n",
    "    dt_grid_results[suffix] = {}\n",
    "    \n",
    "    if method_index == 0:\n",
    "        clf_param_name = 'estimator'\n",
    "    else:\n",
    "        clf_param_name = 'classifier'\n",
    "        \n",
    "    dt_param_grid = {\n",
    "        'clf__' + clf_param_name + '__criterion' : ['gini', 'entropy'],\n",
    "        'clf__' + clf_param_name + '__max_depth' : [i for i in range(2, 8)],\n",
    "        'clf__' + clf_param_name + '__min_samples_split' : [round(val, 2) for val in np.linspace(0.1, 0.5, 5)],\n",
    "        'clf__' + clf_param_name + '__min_samples_leaf' : [0.1, 0.2],\n",
    "        'clf__' + clf_param_name + '__max_features' : [\"sqrt\", \"auto\"],\n",
    "        'tfidf_pipeline__tfidf_vectorizer__ngram_range' : [(1,1), (1,2), (1,3)]\n",
    "    }\n",
    "    \n",
    "    cv_split = ShuffleSplit(n_splits = 3, test_size = .1, random_state = 42)\n",
    "    \n",
    "    dt_grid_search = GridSearchCV(\n",
    "        estimator=dt_grid_pipeline[method_index], param_grid=dt_param_grid,\n",
    "        scoring='accuracy', cv=cv_split, verbose=1, n_jobs=-1, refit=True\n",
    "    )\n",
    "\n",
    "    if method_index == 0:\n",
    "        class_predictions = []\n",
    "        test_predictions = []\n",
    "        for label in output_labels:\n",
    "            dt_grid_results[suffix + '_' + label] = {}\n",
    "            print('... Processing {}'.format(label))\n",
    "            dt_grid_search.fit(X_train, np.array(data[label]))\n",
    "            print('Train Score : ', dt_grid_search.score(X_train, np.array(data[label])))\n",
    "            \n",
    "            dt_grid_results[suffix + '_' + label].update({'train_score' : dt_grid_search.score(X_train, np.array(data[label]))})\n",
    "            dt_grid_results[suffix + '_' + label].update({'best_params' : dt_grid_search.best_params_})\n",
    "            dt_grid_results[suffix + '_' + label].update({'best_estimator_score' : dt_grid_search.best_estimator_.score(X_train, np.array(data[label]))})\n",
    "            \n",
    "            class_predictions.append(dt_grid_search.predict(X_train))\n",
    "            test_predictions.append(dt_grid_search.predict(X_test))\n",
    "            \n",
    "        pd.DataFrame(np.array(class_predictions)).T.to_csv(suffix + '_' + \"OVR.csv\")\n",
    "        pd.DataFrame(np.array(test_predictions)).T.to_csv(\"test_pred_\" + suffix + '_' + \"OVR.csv\")\n",
    "        dt_grid_results[suffix + '_' + 'OVR'] = {}\n",
    "        dt_grid_results[suffix + '_' + 'OVR'].update({'full_train_score' : calculate_OVR_overall_accuracy(y_train, np.array(class_predictions).T)})\n",
    "            \n",
    "    else:\n",
    "        dt_grid_results[suffix] = {}\n",
    "        dt_grid_search.fit(X_train, y_train)\n",
    "        dt_grid_results[suffix].update({'train_score' : dt_grid_search.score(X_train, y_train)})\n",
    "        dt_grid_results[suffix].update({'best_score' : dt_grid_search.best_score_})\n",
    "        dt_grid_results[suffix].update({'best_params' : dt_grid_search.best_params_})\n",
    "        dt_grid_results[suffix].update({'train_pred' : dt_grid_search.predict(X_train)})\n",
    "        dt_grid_results[suffix].update({'F1 Score' : f1_score(y_train, dt_grid_results[suffix]['train_pred'], average='micro')})\n",
    "        dt_grid_results[suffix].update({'Precision' : precision_score(y_train, dt_grid_results[suffix]['train_pred'])})\n",
    "        dt_grid_results[suffix].update({'Recall' : recall_score(y_train, dt_grid_results[suffix]['train_pred'])})\n",
    "                                              \n",
    "        dt_grid_results[suffix].update({'best_estimator_score' : dt_grid_search.best_estimator_.score(X_train, y_train)})\n",
    "        \n",
    "        try:\n",
    "            pd.DataFrame.sparse.from_spmatrix(dt_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "        except:\n",
    "            print('Sparse failed for ',suffix)\n",
    "            try:\n",
    "                pd.DataFrame(dt_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "            except:\n",
    "                print('Normal CSV also failed for', suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "algorithm_index = 3\n",
    "\n",
    "rf_grid_results = {}\n",
    "rf_grid_pipeline = get_baseline_pipeline(algorithm_index)\n",
    "\n",
    "for method_index in range(len(methods)): # CHange 1 to zero\n",
    "    suffix = 'Grid_' + ML_algo_names[algorithm_index] + '_' + methods[method_index]\n",
    "    print(suffix)\n",
    "    \n",
    "    rf_grid_results[suffix] = {}\n",
    "    \n",
    "    if method_index == 0:\n",
    "        clf_param_name = 'estimator'\n",
    "    else:\n",
    "        clf_param_name = 'classifier'\n",
    "        \n",
    "    rf_param_grid = {\n",
    "        'clf__' + clf_param_name + '__bootstrap': [True],\n",
    "        'clf__' + clf_param_name + '__criterion' : ['gini'],\n",
    "        'clf__' + clf_param_name + '__n_estimators' : [1000, 1500, 2000],\n",
    "        'clf__' + clf_param_name + '__min_samples_split' : [0.3, 0.4, 0.5],\n",
    "        'clf__' + clf_param_name + '__min_samples_leaf' : [0.1, 0.2],\n",
    "        'clf__' + clf_param_name + '__max_features' : [\"log2\", \"auto\"],\n",
    "        'tfidf_pipeline__tfidf_vectorizer__ngram_range' : [(1,1), (1,2), (1,3)]\n",
    "    }\n",
    "    \n",
    "    cv_split = ShuffleSplit(n_splits = 3, test_size = .1, random_state = 42)\n",
    "    \n",
    "    rf_grid_search = GridSearchCV(\n",
    "        estimator=rf_grid_pipeline[method_index], param_grid=rf_param_grid,\n",
    "        scoring='accuracy', cv=cv_split, verbose=1, n_jobs=-1, refit=True\n",
    "    )\n",
    "\n",
    "    if method_index == 0:\n",
    "        class_predictions = []\n",
    "        test_predictions = []\n",
    "        for label in output_labels:\n",
    "            rf_grid_results[suffix + '_' + label] = {}\n",
    "            print('... Processing {}'.format(label))\n",
    "            rf_grid_search.fit(X_train, np.array(data[label]))\n",
    "            print('Train Score : ', rf_grid_search.score(X_train, np.array(data[label])))\n",
    "            \n",
    "            rf_grid_results[suffix + '_' + label].update({'train_score' : rf_grid_search.score(X_train, np.array(data[label]))})\n",
    "            rf_grid_results[suffix + '_' + label].update({'best_params' : rf_grid_search.best_params_})\n",
    "            rf_grid_results[suffix + '_' + label].update({'best_estimator_score' : rf_grid_search.best_estimator_.score(X_train, np.array(data[label]))})\n",
    "            \n",
    "            class_predictions.append(rf_grid_search.predict(X_train))\n",
    "            test_predictions.append(rf_grid_search.predict(X_test))\n",
    "        \n",
    "        pd.DataFrame(np.array(class_predictions)).T.to_csv(suffix + '_' + \"OVR.csv\")\n",
    "        pd.DataFrame(np.array(test_predictions)).T.to_csv(\"test_pred_\" + suffix + '_' + \"OVR.csv\")\n",
    "        rf_grid_results[suffix + '_' + 'OVR'] = {}\n",
    "        rf_grid_results[suffix + '_' + 'OVR'].update({'full_train_score' : calculate_OVR_overall_accuracy(y_train, np.array(class_predictions).T)})\n",
    "            \n",
    "    else:\n",
    "        rf_grid_results[suffix] = {}\n",
    "        rf_grid_search.fit(X_train, y_train)\n",
    "        rf_grid_results[suffix].update({'train_score' : rf_grid_search.score(X_train, y_train)})\n",
    "        rf_grid_results[suffix].update({'best_score' : rf_grid_search.best_score_})\n",
    "        rf_grid_results[suffix].update({'best_params' : rf_grid_search.best_params_})\n",
    "        rf_grid_results[suffix].update({'train_pred' : rf_grid_search.predict(X_train)})\n",
    "        rf_grid_results[suffix].update({'F1 Score' : f1_score(y_train, rf_grid_results[suffix]['train_pred'], average='micro')})\n",
    "        rf_grid_results[suffix].update({'Precision' : precision_score(y_train, rf_grid_results[suffix]['train_pred'])})\n",
    "        rf_grid_results[suffix].update({'Recall' : recall_score(y_train, rf_grid_results[suffix]['train_pred'])})\n",
    "                                              \n",
    "        rf_grid_results[suffix].update({'best_estimator_score' : rf_grid_search.best_estimator_.score(X_train, y_train)})\n",
    "        \n",
    "        try:\n",
    "            pd.DataFrame.sparse.from_spmatrix(rf_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "        except:\n",
    "            print('Sparse failed for ',suffix)\n",
    "            try:\n",
    "                pd.DataFrame(rf_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "            except:\n",
    "                print('Normal CSV also failed for', suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Linear SVM\n",
    "\n",
    "algorithm_index = 4\n",
    "\n",
    "svc_grid_results = {}\n",
    "svc_grid_pipeline = get_baseline_pipeline(algorithm_index)\n",
    "\n",
    "for method_index in range(len(methods)): # CHange 1 to zero\n",
    "    suffix = 'Grid_' + ML_algo_names[algorithm_index] + '_' + methods[method_index]\n",
    "    print(suffix)\n",
    "    \n",
    "    svc_grid_results[suffix] = {}\n",
    "    \n",
    "    if method_index == 0:\n",
    "        clf_param_name = 'estimator'\n",
    "    else:\n",
    "        clf_param_name = 'classifier'\n",
    "        \n",
    "        'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [10, 1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf']\n",
    "                \n",
    "    svc_param_grid = {\n",
    "        'clf__' + clf_param_name + '__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'clf__' + clf_param_name + '__gamma' : [10, 1, 0.1, 0.01, 0.001],\n",
    "        'clf__' + clf_param_name + '__kernel' : ['rbf'],\n",
    "        'tfidf_pipeline__tfidf_vectorizer__ngram_range' : [(1,1), (1,2), (1,3)]\n",
    "    }\n",
    "    \n",
    "    cv_split = ShuffleSplit(n_splits = 3, test_size = .1, random_state = 42)\n",
    "    \n",
    "    svc_grid_search = GridSearchCV(\n",
    "        estimator=svc_grid_pipeline[method_index], param_grid=svc_param_grid,\n",
    "        scoring='accuracy', cv=cv_split, verbose=1, n_jobs=-1, refit=True\n",
    "    )\n",
    "\n",
    "    if method_index == 0:\n",
    "        class_predictions = []\n",
    "        test_predictions = []\n",
    "        for label in output_labels:\n",
    "            svc_grid_results[suffix + '_' + label] = {}\n",
    "            print('... Processing {}'.format(label))\n",
    "            svc_grid_search.fit(X_train, np.array(data[label]))\n",
    "            print('Train Score : ', svc_grid_search.score(X_train, np.array(data[label])))\n",
    "            \n",
    "            svc_grid_results[suffix + '_' + label].update({'train_score' : svc_grid_search.score(X_train, np.array(data[label]))})\n",
    "            svc_grid_results[suffix + '_' + label].update({'best_params' : svc_grid_search.best_params_})\n",
    "            svc_grid_results[suffix + '_' + label].update({'best_estimator_score' : svc_grid_search.best_estimator_.score(X_train, np.array(data[label]))})\n",
    "            \n",
    "            class_predictions.append(svc_grid_search.predict(X_train))\n",
    "            test_predictions.append(svc_grid_search.predict(X_test))\n",
    "            \n",
    "        pd.DataFrame(np.array(class_predictions)).T.to_csv(suffix + '_' + \"OVR.csv\")\n",
    "        pd.DataFrame(np.array(test_predictions)).T.to_csv(\"test_pred_\" + suffix + '_' + \"OVR.csv\")\n",
    "        svc_grid_results[suffix + '_' + 'OVR'] = {}\n",
    "        svc_grid_results[suffix + '_' + 'OVR'].update({'full_train_score' : calculate_OVR_overall_accuracy(y_train, np.array(class_predictions).T)})\n",
    "            \n",
    "    else:\n",
    "        svc_grid_results[suffix] = {}\n",
    "        svc_grid_search.fit(X_train, y_train)\n",
    "        svc_grid_results[suffix].update({'train_score' : svc_grid_search.score(X_train, y_train)})\n",
    "        svc_grid_results[suffix].update({'best_score' : svc_grid_search.best_score_})\n",
    "        svc_grid_results[suffix].update({'best_params' : svc_grid_search.best_params_})\n",
    "        svc_grid_results[suffix].update({'train_pred' : svc_grid_search.predict(X_train)})\n",
    "        svc_grid_results[suffix].update({'F1 Score' : f1_score(y_train, svc_grid_results[suffix]['train_pred'], average='micro')})\n",
    "        svc_grid_results[suffix].update({'Precision' : precision_score(y_train, svc_grid_results[suffix]['train_pred'])})\n",
    "        svc_grid_results[suffix].update({'Recall' : recall_score(y_train, svc_grid_results[suffix]['train_pred'])})\n",
    "                                              \n",
    "        svc_grid_results[suffix].update({'best_estimator_score' : svc_grid_search.best_estimator_.score(X_train, y_train)})\n",
    "        \n",
    "        try:\n",
    "            pd.DataFrame.sparse.from_spmatrix(svc_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "        except:\n",
    "            print('Sparse failed for ',suffix)\n",
    "            try:\n",
    "                pd.DataFrame(svc_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "            except:\n",
    "                print('Normal CSV also failed for', suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM\n",
    "\n",
    "algorithm_index = 4\n",
    "\n",
    "svc_linear_grid_results = {}\n",
    "svc_linear_grid_pipeline = get_baseline_pipeline(algorithm_index)\n",
    "\n",
    "for method_index in range(len(methods)): # CHange 1 to zero\n",
    "    suffix = 'Grid_' + ML_algo_names[algorithm_index] + '_' + methods[method_index]\n",
    "    print(suffix)\n",
    "    \n",
    "    svc_linear_grid_results[suffix] = {}\n",
    "    \n",
    "    if method_index == 0:\n",
    "        clf_param_name = 'estimator'\n",
    "    else:\n",
    "        clf_param_name = 'classifier'\n",
    "        \n",
    "        'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [10, 1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf']\n",
    "                \n",
    "    svc_linear_param_grid = {\n",
    "        'clf__' + clf_param_name + '__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "        'tfidf_pipeline__tfidf_vectorizer__ngram_range' : [(1,1), (1,2), (1,3)]\n",
    "    }\n",
    "    \n",
    "    cv_split = ShuffleSplit(n_splits = 3, test_size = .1, random_state = 42)\n",
    "    \n",
    "    svc_linear_grid_search = GridSearchCV(\n",
    "        estimator=svc_linear_grid_pipeline[method_index], param_grid=svc_linear_param_grid,\n",
    "        scoring='accuracy', cv=cv_split, verbose=1, n_jobs=-1, refit=True\n",
    "    )\n",
    "\n",
    "    if method_index == 0:\n",
    "        class_predictions = []\n",
    "        test_predictions = []\n",
    "        for label in output_labels:\n",
    "            svc_linear_grid_results[suffix + '_' + label] = {}\n",
    "            print('... Processing {}'.format(label))\n",
    "            svc_linear_grid_search.fit(X_train, np.array(data[label]))\n",
    "            print('Train Score : ', svc_linear_grid_search.score(X_train, np.array(data[label])))\n",
    "            \n",
    "            svc_linear_grid_results[suffix + '_' + label].update({'train_score' : svc_linear_grid_search.score(X_train, np.array(data[label]))})\n",
    "            svc_linear_grid_results[suffix + '_' + label].update({'best_params' : svc_linear_grid_search.best_params_})\n",
    "            svc_linear_grid_results[suffix + '_' + label].update({'best_estimator_score' : svc_linear_grid_search.best_estimator_.score(X_train, np.array(data[label]))})\n",
    "            \n",
    "            class_predictions.append(svc_linear_grid_search.predict(X_train))\n",
    "            test_predictions.append(svc_linear_grid_search.predict(X_test))\n",
    "            \n",
    "        pd.DataFrame(np.array(class_predictions)).T.to_csv(suffix + '_' + \"OVR.csv\")\n",
    "        pd.DataFrame(np.array(test_predictions)).T.to_csv(\"test_pred_\" + suffix + '_' + \"OVR.csv\")\n",
    "        svc_linear_grid_results[suffix + '_' + 'OVR'] = {}\n",
    "        svc_linear_grid_results[suffix + '_' + 'OVR'].update({'full_train_score' : calculate_OVR_overall_accuracy(y_train, np.array(class_predictions).T)})\n",
    "\n",
    "    else:\n",
    "        svc_linear_grid_results[suffix] = {}\n",
    "        svc_linear_grid_search.fit(X_train, y_train)\n",
    "        svc_linear_grid_results[suffix].update({'train_score' : svc_linear_grid_search.score(X_train, y_train)})\n",
    "        svc_linear_grid_results[suffix].update({'best_score' : svc_linear_grid_search.best_score_})\n",
    "        svc_linear_grid_results[suffix].update({'best_params' : svc_linear_grid_search.best_params_})\n",
    "        svc_linear_grid_results[suffix].update({'train_pred' : svc_linear_grid_search.predict(X_train)})\n",
    "        svc_linear_grid_results[suffix].update({'F1 Score' : f1_score(y_train, svc_linear_grid_results[suffix]['train_pred'], average='micro')})\n",
    "        svc_linear_grid_results[suffix].update({'Precision' : precision_score(y_train, svc_linear_grid_results[suffix]['train_pred'])})\n",
    "        svc_linear_grid_results[suffix].update({'Recall' : recall_score(y_train, svc_linear_grid_results[suffix]['train_pred'])})\n",
    "                                              \n",
    "        svc_linear_grid_results[suffix].update({'best_estimator_score' : svc_linear_grid_search.best_estimator_.score(X_train, y_train)})\n",
    "        \n",
    "        try:\n",
    "            pd.DataFrame.sparse.from_spmatrix(svc_linear_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "        except:\n",
    "            print('Sparse failed for ',suffix)\n",
    "            try:\n",
    "                pd.DataFrame(svc_linear_grid_search.predict(X_test)).to_csv(\"test_pred_\" + suffix + \".csv\")\n",
    "            except:\n",
    "                print('Normal CSV also failed for', suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rBTYPf1ygR9"
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8gPokD4yjos"
   },
   "outputs": [],
   "source": [
    "w2v_path = '/content/drive/My Drive/AV_Independence/Word2Vec Models/'\n",
    "W2V_names = ['model_128_sg', 'model_128_cbow', 'model_256_sg', 'model_256_cbow']\n",
    "\n",
    "#google_wv_file = '/content/drive/My Drive/AV_Independence/GoogleNews-vectors-negative300.bin.gz'\n",
    "#google_wv = KeyedVectors.load_word2vec_format(google_wv_file, binary=True)\n",
    "\n",
    "def get_Word2Vec_models(X_train):\n",
    "    model_128_sg = Word2Vec(sentences=X_train, size=128,\n",
    "                          sg=1, window=10, iter=5,\n",
    "                          min_count=2, workers=4)\n",
    "    model_128_sg.save(w2v_path + 'model_128_sg.model')\n",
    "\n",
    "    model_128_cbow = Word2Vec(sentences=X_train, size=128,\n",
    "                          sg=0, window=5, iter=5,\n",
    "                          min_count=2, workers=4)\n",
    "    model_128_cbow.save(w2v_path + 'model_128_cbow.model')\n",
    "\n",
    "    model_256_sg = Word2Vec(sentences=X_train, size=256,\n",
    "                          sg=1, window=10, iter=5,\n",
    "                          min_count=2, workers=4)\n",
    "    model_256_sg.save(w2v_path + 'model_256_sg.model')\n",
    "\n",
    "    model_256_cbow = Word2Vec(sentences=X_train, size=256,\n",
    "                          sg=0, window=5, iter=5,\n",
    "                          min_count=2, workers=4)\n",
    "    model_256_cbow.save(w2v_path + 'model_256_cbow.model')\n",
    "\n",
    "    return [model_128_sg, model_128_cbow, model_256_sg, model_256_cbow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_models = get_Word2Vec_models(X_train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEF4c5pdwBt7"
   },
   "outputs": [],
   "source": [
    "def CNN_model(embedding_layer):\n",
    "  \n",
    "    drop = 0.2\n",
    "\n",
    "    CNN = Sequential()\n",
    "    CNN.add(embedding_layer)\n",
    "\n",
    "    CNN.add(Conv1D(filters=128, kernel_size=5))\n",
    "    #CNN.add(BatchNormalization())\n",
    "    CNN.add(Activation('relu'))\n",
    "    CNN.add(MaxPooling1D(pool_size=2))\n",
    "    CNN.add(Dropout(drop))\n",
    "\n",
    "    CNN.add(Conv1D(filters=128, kernel_size=7))\n",
    "    #CNN.add(BatchNormalization())\n",
    "    CNN.add(Activation('relu'))\n",
    "    CNN.add(MaxPooling1D(pool_size=2))\n",
    "    CNN.add(Dropout(drop))\n",
    "\n",
    "    CNN.add(Conv1D(filters=128, kernel_size=5))\n",
    "    #CNN.add(BatchNormalization())\n",
    "    CNN.add(Activation('relu'))\n",
    "    CNN.add(MaxPooling1D(pool_size=2))\n",
    "    CNN.add(Dropout(drop))\n",
    "\n",
    "    CNN.add(Flatten())\n",
    "\n",
    "    CNN.add(Dense(256))\n",
    "    #CNN.add(BatchNormalization())\n",
    "    CNN.add(Activation('relu'))\n",
    "    CNN.add(Dropout(drop))\n",
    "\n",
    "    CNN.add(Dense(256))\n",
    "    #CNN.add(BatchNormalization())\n",
    "    CNN.add(Activation('relu'))\n",
    "    CNN.add(Dropout(drop))\n",
    "\n",
    "    CNN.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "    print(CNN.summary())\n",
    "\n",
    "    return CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XQO5VGY9Lgg"
   },
   "source": [
    "### CNN Multi Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_WLQ_XRwByt"
   },
   "outputs": [],
   "source": [
    "def CNN_multi(embedding_layer, max_length):\n",
    "  \n",
    "    drop_embed = 0.2 \n",
    "\n",
    "    n_conv_1 = n_conv_2 = n_conv_3 = 128 \n",
    "    k_conv_1 = 3\n",
    "    k_conv_2 = 5\n",
    "    k_conv_3 = 4\n",
    "\n",
    "    n_dense = 256\n",
    "    dropout = 0.2\n",
    "\n",
    "    input_layer = Input(shape=(max_length,), name='input')\n",
    "\n",
    "    embedding_layer = embedding_layer(input_layer)\n",
    "    drop_embed_layer = SpatialDropout1D(drop_embed, name='drop_embed')(embedding_layer)\n",
    "\n",
    "    # three parallel convolutional streams: \n",
    "    conv_1 = Conv1D(n_conv_1, k_conv_1, activation='relu', name='conv_1')(drop_embed_layer)\n",
    "    maxp_1 = GlobalMaxPooling1D(name='maxp_1')(conv_1)\n",
    "\n",
    "    conv_2 = Conv1D(n_conv_2, k_conv_2, activation='relu', name='conv_2')(drop_embed_layer)\n",
    "    maxp_2 = GlobalMaxPooling1D(name='maxp_2')(conv_2)\n",
    "\n",
    "    conv_3 = Conv1D(n_conv_3, k_conv_3, activation='relu', name='conv_3')(drop_embed_layer)\n",
    "    maxp_3 = GlobalMaxPooling1D(name='maxp_3')(conv_3)\n",
    "\n",
    "    # concatenate the activations from the three streams: \n",
    "    concat = concatenate([maxp_1, maxp_2, maxp_3])\n",
    "\n",
    "    dense_layer = Dense(n_dense, activation='relu', name='dense')(concat)\n",
    "    drop_dense_layer = Dropout(dropout, name='drop_dense')(dense_layer)\n",
    "    dense_2 = Dense(int(n_dense/2), activation='relu', name='dense_2')(drop_dense_layer)\n",
    "    dropout_2 = Dropout(dropout, name='drop_dense_2')(dense_2)\n",
    "\n",
    "    predictions = Dense(6, activation='sigmoid', name='output')(dropout_2)\n",
    "\n",
    "    model = Model(input_layer, predictions)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBhl83FcY8Ql"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmcPUYzDY96n"
   },
   "outputs": [],
   "source": [
    "def LSTM_model(embedding_layer):\n",
    "    drop = 0.2\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(128, dropout=drop))) #add dropout\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN + LSTM Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_show(embedding_layer):\n",
    "    drop = 0.2\n",
    "\n",
    "    CNN = Sequential()\n",
    "    CNN.add(embedding_layer)\n",
    "\n",
    "    CNN.add(Conv1D(filters=128, kernel_size=5))\n",
    "    #CNN.add(BatchNormalization())\n",
    "    CNN.add(Activation('relu'))\n",
    "    CNN.add(MaxPooling1D(pool_size=2))\n",
    "    CNN.add(Dropout(drop))\n",
    "\n",
    "    CNN.add(Conv1D(filters=128, kernel_size=7))\n",
    "    #CNN.add(BatchNormalization())\n",
    "    CNN.add(Activation('relu'))\n",
    "    CNN.add(MaxPooling1D(pool_size=2))\n",
    "    CNN.add(Dropout(drop))\n",
    "\n",
    "    CNN.add(Conv1D(filters=128, kernel_size=5))\n",
    "    #CNN.add(BatchNormalization())\n",
    "    CNN.add(Activation('relu'))\n",
    "    CNN.add(MaxPooling1D(pool_size=2))\n",
    "    CNN.add(Dropout(drop))\n",
    "\n",
    "    CNN.add(Flatten())\n",
    "\n",
    "    CNN.add(Bidirectional(LSTM(256, dropout=drop)))\n",
    "\n",
    "    CNN.add(Dense(256))\n",
    "    #CNN.add(BatchNormalization())\n",
    "    CNN.add(Activation('relu'))\n",
    "    CNN.add(Dropout(drop))\n",
    "\n",
    "    CNN.add(Dense(256))\n",
    "    #CNN.add(BatchNormalization())\n",
    "    CNN.add(Activation('relu'))\n",
    "    CNN.add(Dropout(drop))\n",
    "\n",
    "    CNN.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "    print(CNN.summary())\n",
    "\n",
    "    return CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general flow for all deep learning algorithms are as follows:\n",
    "\n",
    "> Repeat until model shows no improvement\n",
    ">> for every Word2Vec model:\n",
    ">>> Train and validate the DL algorithm\\\n",
    "Tune and modify the hyperparameters\n",
    "\n",
    "This has been done for both the Word2Vec models learnt from the data and also using pretrained GloVe models.\n",
    "\n",
    "If a Word2Vec model performs poorly across several hyperparameter settings, it is eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1-v_qf902Ta"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(X_train):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_text(text, tokenizer, max_length):\n",
    "    encoded_text = tokenizer.texts_to_sequences(text)\n",
    "    padded_text = pad_sequences(encoded_text, maxlen=max_length, padding='post')\n",
    "    return padded_text\n",
    "\n",
    "def get_tokenizer_embedding_mapper(embedding, tokenizer_word_index):\n",
    "    not_present = []\n",
    "    vocab_size = len(tokenizer_word_index) + 1\n",
    "    tokenizer_embedding_mapper = np.zeros((vocab_size, embedding.wv.vector_size))\n",
    "    for word, index in tokenizer_word_index.items():\n",
    "        try:\n",
    "            tokenizer_embedding_mapper[index] = embedding[word]\n",
    "        except:\n",
    "            not_present.append(word)\n",
    "            \n",
    "    return tokenizer_embedding_mapper, not_present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T13:53:38.331689Z",
     "start_time": "2020-08-30T13:53:38.325704Z"
    }
   },
   "source": [
    "Below is an example to train the first shown CNN model. The same can be applied for other alogirthms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec learnt from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_names = ['model_128_sg', 'model_128_cbow', 'model_256_sg', 'model_256_cbow']\n",
    "for W2V_model_name in W2V_names:\n",
    "\n",
    "    W2V_model = Word2Vec.load(w2v_path + W2V_model_name + '.model')\n",
    "\n",
    "    tokenizer = create_tokenizer(X_train_dl) #features of X_train\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_length = max([len(headline) for headline in X_train_dl])\n",
    "\n",
    "    # Encode text information into numeric values\n",
    "    X_train_dl_encoded = encode_text(X_train_dl, tokenizer, max_length)\n",
    "    X_valid_dl_encoded = encode_text(X_valid_dl, tokenizer, max_length)\n",
    "    X_test_dl_encoded = encode_text(X_test_dl, tokenizer, max_length)\n",
    "\n",
    "    # Linking the learned embedding to the keras tokenizer object.\n",
    "    tokenizer_embedding_mapper, oov = get_tokenizer_embedding_mapper(W2V_model, tokenizer.word_index)\n",
    "    print('\\nTokenizer Shape : ', tokenizer_embedding_mapper.shape)\n",
    "\n",
    "    print('\\nDimension Cnecking : ', W2V_model_name, len(oov) + len(W2V_model.wv.vocab) == len(tokenizer.word_index)) #filename from #1#\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, W2V_model.wv.vector_size, weights=[tokenizer_embedding_mapper], input_length=max_length, trainable=True) \t# Possi Trainable\n",
    "\n",
    "    model = CNN_model(embedding_layer)\n",
    "\n",
    "    print('############## COMPILING ' + W2V_model_name + '###############')\n",
    "\n",
    "    model_name = \"CNN_iteration_1\" + W2V_model_name\n",
    "\n",
    "    checkpoint_file = \"weights_\" + model_name + \".h5\"\n",
    "    checkpoint = ModelCheckpoint(checkpoint_file, monitor='val_accuracy',\n",
    "                               save_best_only=True, mode='max', verbose=1\n",
    "                               )\n",
    "    callbacks = [checkpoint]\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0002, beta_1=0.95, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(np.array(X_train_dl_encoded), np.array(y_train_dl),\n",
    "            batch_size=32, epochs=15, verbose=2,\n",
    "            validation_data=(np.array(X_valid_dl_encoded), np.array(y_valid_dl)),\n",
    "            callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxWgyl3pDDyZ"
   },
   "source": [
    "### Glove Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_names = ['glove.6B.100d.txt', 'glove.6B.200d.txt']\n",
    "length = 100\n",
    "\n",
    "for W2V_model_name in W2V_names:\n",
    "  \n",
    "    glove_file = open(source_path + W2V_model_name)\n",
    "    embeddings_dictionary = dict()\n",
    "\n",
    "    for line in glove_file:\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "        vector = np.array(line[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector\n",
    "    glove_file.close()\n",
    "\n",
    "    tokenizer = create_tokenizer(X_train_dl)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_length = max([len(headline) for headline in X_train_dl])\n",
    "\n",
    "    X_train_dl_encoded = encode_text(X_train_dl, tokenizer, max_length)\n",
    "    X_valid_dl_encoded = encode_text(X_valid_dl, tokenizer, max_length)\n",
    "    X_test_dl_encoded = encode_text(X_test_dl, tokenizer, max_length)\n",
    "\n",
    "    tokenizer_embedding_mapper = np.zeros((vocab_size, length))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            tokenizer_embedding_mapper[index] = embedding_vector\n",
    "\n",
    "    # Linking the learned embedding to the keras tokenizer object.\n",
    "    print('\\nTokenizer Shape : ', tokenizer_embedding_mapper.shape)\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, length, weights=[tokenizer_embedding_mapper], input_length=max_length, trainable=False) \t# Possi Trainable\n",
    "\n",
    "    model = CNN_model(embedding_layer)\n",
    "\n",
    "    print('############## COMPILING ' + W2V_model_name + '###############')\n",
    "\n",
    "    model_name = \"CNN_Glove_iteration_1\" + W2V_model_name\n",
    "\n",
    "    checkpoint_file = \"weights_\" + model_name + \".h5\"\n",
    "    checkpoint = ModelCheckpoint(checkpoint_file, monitor='val_accuracy',\n",
    "                                save_best_only=True, mode='max', verbose=1\n",
    "                                )\n",
    "    callbacks = [checkpoint]\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0002, beta_1=0.95, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(np.array(X_train_dl_encoded), np.array(y_train_dl),\n",
    "            batch_size=64, epochs=8, verbose=2,\n",
    "            validation_data=(np.array(X_valid_dl_encoded), np.array(y_valid_dl)),\n",
    "            callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    length = length + 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance is measured using micro F1 Score. The best results obtained for each of the above models are as follows:\n",
    "\n",
    "| S.No | Model | Best Test Public | Best Test Private |\n",
    "|:- | :- | :-: | :-: |\n",
    "| 1 | RNN | 0.8286 | 0.8367\n",
    "| 2 | CNN + LSTM | 0.8279 | 0.8302\n",
    "| 3 | CNN | 0.8282 | 0.8289"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
